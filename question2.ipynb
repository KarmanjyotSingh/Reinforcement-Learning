{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Taxi-v3 (Monte Carlo methods and TD methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_name = 'Taxi-v3'\n",
    "render_mode = 'rgb_array'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Monte Carlo Methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On-Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Off-Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal-Difference Learning Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDAgent:\n",
    "    # An agent class that interacts with the environment\n",
    "    def __init__(self,actionSpace = 4,stateSpace = 48) -> None:\n",
    "        # parameter space for the agent\n",
    "        self.actionSpace = actionSpace # number of actions\n",
    "        self.stateSpace = stateSpace   # number of states\n",
    "        self.e = 0.1                   # epsilon\n",
    "        self.discount_factor = 0.95    # gamma\n",
    "        self.learning_rate = 0.8       # alpha \n",
    "        self.Q_values = np.zeros((stateSpace,actionSpace),dtype=np.float32) # Q values\n",
    "    \n",
    "    def epsilon_greedy(self,s):\n",
    "        # epsilon greedy policy\n",
    "        epsilon = random.random() # choose a random number between 0 and 1\n",
    "        # compare the epsilon with the threshold\n",
    "        if epsilon < self.e: \n",
    "            #  if epsilon is less than the threshold, choose a random action from the action space\n",
    "            return np.random.choice(self.actionSpace)  \n",
    "        else:\n",
    "            # if epsilon is greater than the threshold, choose the action with the highest Q value\n",
    "            return np.argmax(self.Q_values[s,]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDLearning:\n",
    "    # TD learning experiment\n",
    "    # initialise the parameters for the TD learning experiment\n",
    "    def __init__(self,num_episodes = 500,method = 'Q-Learning') -> None:\n",
    "\n",
    "        self.num_episodes = num_episodes # number of episodes\n",
    "        self.episode_length = 500        # maximum length of an episode\n",
    "        self.method = method             # method to use for TD learning (Q-Learning or SARSA)\n",
    "        \n",
    "\n",
    "    # this is the main TD experiment\n",
    "    def run(self,env,agent):\n",
    "        # stats store the accumulated reward for each episode\n",
    "        stats = np.zeros(self.num_episodes)\n",
    "        #  episode_lengths store the length of each episode\n",
    "        episode_lengths = np.zeros(self.num_episodes) \n",
    "        \n",
    "        # this is a helper function\n",
    "        # this is used to update the Q-Values in the TD learning algorithm\n",
    "        def update_q_value(state,a,next_state,reward,agent):\n",
    "            \n",
    "            Q_values = agent.Q_values\n",
    "            q = Q_values[state,a]\n",
    "            \n",
    "            optimal_target = 0\n",
    "            if self.method == 'Q-Learning': # pick the action greedily w.r.t the Q values\n",
    "                optimal_target = agent.discount_factor*np.max(Q_values[next_state,])\n",
    "            elif self.method == 'SARSA': # use the epsilon greedy policy to pick the next action\n",
    "                optimal_target = agent.discount_factor*Q_values[next_state,agent.epsilon_greedy(next_state)]\n",
    "            # update the Q value\n",
    "            q = q + agent.learning_rate*(reward+optimal_target - q)\n",
    "\n",
    "            return q\n",
    "\n",
    "        # helper function to print progress after every 10000 episodes\n",
    "        def print_progress(episode):\n",
    "            print(\"Episode: {}/{}\".format(episode, self.num_episodes), end = \" \")\n",
    "            print(\"Cumulative Reward: {}\".format(stats[episode]), end='\\n')\n",
    "            \n",
    "        # run the experiment iterate over the number of episodes\n",
    "        for episodes in range(self.num_episodes):\n",
    "            observation = env.reset() # get the initial observation from the environment\n",
    "            s = observation[0]        # get the state from the observation\n",
    "            # t = 0\n",
    "            for t in range(self.episode_length): # iterate over the episode length\n",
    "                # t+=1\n",
    "                a = agent.epsilon_greedy(s) # choose the action in state s using the epsilon greedy policy\n",
    "                \n",
    "                next_state,reward,done,info , _ = env.step(a) # take the action and get the next state, reward, done and info\n",
    "                stats[episodes]+=reward         # update the cumulative reward\n",
    "                episode_lengths[episodes] = t   # update the episode length\n",
    "                agent.Q_values[s,a] = update_q_value(s,a,next_state,reward,agent) # do the update step\n",
    "                s = next_state                  # update the state\n",
    "\n",
    "                if done == True:\n",
    "                    # reached terminal state\n",
    "                    break\n",
    "                # return 1\n",
    "            if episodes % 100 == 0:\n",
    "                print_progress(episodes)\n",
    "        \n",
    "        return stats,episode_lengths"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(environment_name,render_mode=render_mode)\n",
    "env.reset()\n",
    "nA = env.action_space.n\n",
    "nS = env.observation_space.n \n",
    "\n",
    "agent = TDAgent(actionSpace=nA,stateSpace=nS)\n",
    "qvalue = TDLearning(method='Q-Learning')\n",
    "ql_stats,ql_episode_lengths = qvalue.run(env,agent)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "agent2 = TDAgent(actionSpace=env.action_space.n,stateSpace=env.observation_space.n)\n",
    "SARSA = TDLearning(method='SARSA')\n",
    "sarsa_stats,sarsa_episode_lengths = SARSA.run(env,agent2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "plt.plot(ql_stats,label='Q-Learning')\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Accumulated Reward')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sarsa_stats,label='SARSA')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Accumulated Reward')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the actions in the environment\n",
    "# according to the optimal Q values by the agents\n",
    "\n",
    "\n",
    "def TestAgentPolicy(env,Q_values):\n",
    "    num_episodes = 100\n",
    "    episode_len = 1000\n",
    "    stats = np.zeros(num_episodes)\n",
    "    time = np.zeros(num_episodes)\n",
    "    for episodes in range(num_episodes):\n",
    "        observation = env.reset()\n",
    "        s = observation[0]\n",
    "\n",
    "        for t in range(episode_len):\n",
    "            a = np.argmax(Q_values[s,])\n",
    "            next_state,reward,done,info,_ = env.step(a)\n",
    "            stats[episodes]+=reward\n",
    "            time[episodes] = t\n",
    "            s = next_state\n",
    "            if done == True:\n",
    "                break\n",
    "    return stats,time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot qst \n",
    "\n",
    "qst , tst = TestAgentPolicy(env,agent.Q_values)\n",
    "sst , tst2 = TestAgentPolicy(env,agent2.Q_values)\n",
    "plt.plot(qst,label='Q-Learning')\n",
    "plt.plot(sst,label='SARSA')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Accumulated Reward')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot timesteps tst and tst2\n",
    "plt.plot(tst,label='Q-Learning')\n",
    "plt.plot(tst2,label='SARSA')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Timesteps')\n",
    "# plt.yscale('log')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
