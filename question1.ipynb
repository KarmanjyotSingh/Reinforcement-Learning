{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI CliffWalking-v0 (Value Iteration and Policy Iteration)\n",
    "\n",
    "This "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Helper Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gymnasium \n",
    "# it generates openAI environment for RL Agent\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = 'CliffWalking-v0'\n",
    "renderMode = 'rgb_array'\n",
    "# action mapping \n",
    "actions = {0:'UP', 1:'RIGHT', 2:'DOWN', 3:'LEFT'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Agent\n",
    "\n",
    "A generalised agent class that acts as parent class to the multiple agent classes, that act and behave differently in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        # this initialises the environment\n",
    "        self.env = gym.make(environment,\n",
    "                            render_mode = renderMode)\n",
    "        self.env.reset()\n",
    "\n",
    "        self.state_values = np.zeros(self.env.observation_space.n) # initialise the state values to zero\n",
    "        self.discountFactor = 1 # discount factor\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confused Agent\n",
    "\n",
    "This agent is made such that, it selects the actions from the given action space, randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfusedAgent(Agent):\n",
    "    # This agent picks up random actions \n",
    "    # Based on the action space \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.iterations = 10000\n",
    "        action = self.env.action_space.sample()\n",
    "        return action\n",
    "    \n",
    "    def act(self):\n",
    "        \n",
    "        self.env = gym.make(environment,render_mode = renderMode)\n",
    "        observation, info = self.env.reset()\n",
    "\n",
    "        for _ in range(self.iterations):\n",
    "            action = self.policy(self.env)\n",
    "            observation, reward, terminated, truncated,info = self.env.step(action)\n",
    "\n",
    "            if terminated:\n",
    "                observation, info  = self.env.reset()\n",
    "        self.env.close()    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIterationAgent(Agent):\n",
    "\n",
    "    def __inpassit__(self) -> None:\n",
    "        \n",
    "        super().__init__() # initialise the Agent class\n",
    "        \n",
    "    def PolicyEvaluation(self,policy,threshold = 1e-5)->bool:\n",
    "        \n",
    "        env = self.env               # initialise the environment variable\n",
    "        n = env.observation_space.n  # the number of states \n",
    "        state_values = self.state_values \n",
    "        discountFactor = self.discountFactor\n",
    "        delta = 0   \n",
    "        valueConverged = False\n",
    "        updatedStateValues = state_values.copy()\n",
    "\n",
    "        # iterate over all the states \n",
    "        for state in range(n-1):\n",
    "            # do not iterate over the goal state\n",
    "            # episode terminates at the goal state\n",
    "            stateValue = 0\n",
    "            action = policy[state] # the policy is discrete, get the action\n",
    "            transitionProbabilities = env.P[state][action] # get the transition probabilities for the given state and action\n",
    "\n",
    "            # expected return \n",
    "            for probability, nextState, reward, done in transitionProbabilities:\n",
    "                expected_return =  probability *(reward + discountFactor * updatedStateValues[nextState])\n",
    "                stateValue += expected_return\n",
    "            \n",
    "            delta = max(delta, abs(stateValue - updatedStateValues[state])) # update the delta\n",
    "            updatedStateValues[state] = stateValue\n",
    "\n",
    "        if delta < threshold:\n",
    "            valueConverged = True          \n",
    "        \n",
    "        self.state_values = updatedStateValues\n",
    "\n",
    "        return valueConverged\n",
    "    \n",
    "    def PolicyImprovement(self,policy)->bool:\n",
    "\n",
    "        stop = True\n",
    "        env = self.env\n",
    "        n = env.observation_space.n\n",
    "        discountFactor = self.discountFactor\n",
    "        state_values = self.state_values\n",
    "\n",
    "        for state in range(n-1):\n",
    "            bestAction = 0\n",
    "            bestValue = -np.inf\n",
    "\n",
    "            for a in range(env.action_space.n):\n",
    "                stateValue = 0\n",
    "                transitionProbabilities = env.P[state][a]\n",
    "                for probability, nextState, reward, done in transitionProbabilities:\n",
    "                    expected_return =  probability *(reward + discountFactor * state_values[nextState])\n",
    "                    stateValue += expected_return\n",
    "                \n",
    "                if stateValue > bestValue:\n",
    "                    bestValue = stateValue\n",
    "                    bestAction = a\n",
    "\n",
    "            if policy[state] != bestAction:\n",
    "                stop = False\n",
    "                policy[state] = bestAction\n",
    "        \n",
    "        return stop\n",
    "\n",
    "\n",
    "\n",
    "    def Run(self):\n",
    "\n",
    "        n = self.env.observation_space.n # the number of states\n",
    "        policy = np.zeros(n,dtype = int)\n",
    "\n",
    "        stop = False\n",
    "        iter = 0\n",
    "        while not stop:\n",
    "            # iter += 1\n",
    "            # print(f'Iteration: {iter}') \n",
    "            # run Policy Evaluation           \n",
    "            ValueConverged = False\n",
    "            i = 0         \n",
    "            while not ValueConverged:\n",
    "                i += 1\n",
    "                ValueConverged = self.PolicyEvaluation(policy)\n",
    "                # print(f'Policy Evaluation: {i}')\n",
    "            stop = self.PolicyImprovement(policy)\n",
    "        \n",
    "        return policy, self.state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policyIterationAgent = PolicyIterationAgent()\n",
    "\n",
    "policy, state_values = policyIterationAgent.Run()\n",
    "\n",
    "print('Policy: \\n',policy.reshape(4,12))\n",
    "print('State Values: ',state_values.reshape(4,12))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIteration(Agent):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.policy = np.zeros(self.env.observation_space.n,dtype = int)\n",
    "\n",
    "    \n",
    "    def ValueIteration(self,threshold = 1e-5)->bool:\n",
    "\n",
    "        state_values = self.state_values\n",
    "\n",
    "        n = self.env.observation_space.n\n",
    "\n",
    "        stop = True\n",
    "        delta = 0\n",
    "        for state in range(n-1):\n",
    "            bestValue = -np.inf\n",
    "            \n",
    "            for action in range(self.env.action_space.n):\n",
    "                stateValue = 0\n",
    "                transitionProbabilities = self.env.P[state][action]\n",
    "                for probability, nextState, reward, done in transitionProbabilities:\n",
    "                    expected_return =  probability *(reward + self.discountFactor * state_values[nextState])\n",
    "                    stateValue += expected_return\n",
    "                \n",
    "                if stateValue > bestValue:\n",
    "                    bestValue = stateValue\n",
    "                    self.policy[state] = action\n",
    "            \n",
    "            delta = max(delta, abs(bestValue - state_values[state]))\n",
    "            state_values[state] = bestValue\n",
    "        \n",
    "        self.state_values = state_values\n",
    "\n",
    "        return delta < threshold\n",
    "\n",
    "    def Run(self):\n",
    "\n",
    "        stop = False\n",
    "\n",
    "        while not stop:\n",
    "            stop = self.ValueIteration()\n",
    "        \n",
    "        return self.policy, self.state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valueIteration = ValueIteration()\n",
    "policy, state_values = valueIteration.Run()\n",
    "\n",
    "print('Policy: \\n')\n",
    "policy = policy.reshape(4,12)\n",
    "policy = np.where(policy == 0, 'U', policy)\n",
    "policy = np.where(policy == '1', 'R', policy)\n",
    "policy = np.where(policy == '2', 'D', policy)\n",
    "policy = np.where(policy == '3', 'L', policy)\n",
    "print(policy)\n",
    "print('State Values: \\n',state_values.reshape(4,12))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
